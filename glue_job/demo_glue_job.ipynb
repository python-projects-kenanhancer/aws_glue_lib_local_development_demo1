{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Local Development prerequisites\n",
    "\n",
    "> Run `source ./setup_aws_glue_scripts/setup_aws_glue.sh` in GitBash as administrator.\n",
    "\n",
    "AWS Glue Local Debugging:\n",
    "> Run `./glue_local_debug.sh` or follow the following steps.\n",
    "\n",
    "AWS Glue Interactive Session Debugging:\n",
    "- From the Command Palette (`Ctrl+Shift+P`), select `Tasks: Run Task command`, then select `Convert Notebook to Script`\n",
    "- Run `pipenv shell` in terminal\n",
    "- Run `jupyter notebook --no-browser` in terminal\n",
    "- Copy Jupyter Server url from terminal\n",
    "- From the Command Palette (`Ctrl+Shift+P`), select `Notebook: Select Notebook Kernel`, then select `Select Another Kernel...`, then select `Existing Jupyter Server...`, then paste Jupyter Server url, then enter, then enter again.\n",
    "- Uncomment the following cell, and run it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To configure AWS Glue Interactive Session, uncomment the following cell\n",
    "\n",
    "Change configuration in terms of your requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %profile [your-profile-name]\n",
    "# %region eu-west-1\n",
    "# %iam_role [your-iam-role]\n",
    "# %worker_type G.1X\n",
    "# %number_of_workers 2\n",
    "# %additional_python_modules watchtower\n",
    "# %extra_py_files s3://artifacts-dev/33333/Layers/pip_common_packages_layer_glue.zip,s3://artifacts-dev/33333/Layers/combined_lambda_layer_glue.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To pass system arguments, uncomment the following cell\n",
    "\n",
    "Change arguments in terms of your requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "stage = os.environ.get(\"STAGE\", \"dev\")\n",
    "current_time = datetime.now()\n",
    "\n",
    "sys.argv = [\n",
    "    \"demo_glue_job.py\",  # sys.argv[0], script name\n",
    "    \"--JOB_ID\", \"j_a197e5a4ae431ff7631a35bdcddbb8b6bebf747667c548020bfbd40447569b25\",\n",
    "    \"--JOB_RUN_ID\", \"jr_6a23882f8589db824567693a787ec1014a66dcc868186c2cb8ae26a502f47040\",\n",
    "    \"--JOB_NAME\", \"TEST_JOB\",\n",
    "    \"--execution_arn\", f\"arn:aws:states:eu-west-1:625904187796:execution:TEST_JOB:try-{current_time.strftime('%Y%m%d%H%M%S%f')}\",\n",
    "    \"--job\", \"job_name\",\n",
    "    \"--stage\", stage,\n",
    "    \"--log_group_name\", f\"/aws-glue/jobs/demo_glue_job_{stage}\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize AWS Glue Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\0546dr\\spark-3.3.0-amzn-1-bin-3.3.3-amzn-0\\python\\pyspark\\sql\\context.py:112: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import logging\n",
    "\n",
    "from log_utils import LogUtils\n",
    "\n",
    "from awsglue.transforms import *\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from pyspark.context import SparkContext\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.job import Job\n",
    "\n",
    "\n",
    "# Get parameters passed to the script\n",
    "args = getResolvedOptions(\n",
    "    sys.argv,\n",
    "    [\n",
    "        \"JOB_NAME\",\n",
    "        \"job\",\n",
    "        \"execution_arn\",\n",
    "        \"log_group_name\",\n",
    "        \"stage\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "env = os.environ.get(\"ENV\")\n",
    "log_utils = LogUtils(\n",
    "    log_group_name=args[\"log_group_name\"],\n",
    "    log_stream_name=args[\"JOB_RUN_ID\"],\n",
    "    job_name=args[\"job\"],\n",
    "    execution_arn=args[\"execution_arn\"],\n",
    "    environment=env\n",
    ")\n",
    "\n",
    "log_utils.configure_logging()\n",
    "\n",
    "logger = logging.getLogger()\n",
    "\n",
    "# Initialize Glue context\n",
    "sparkContext = SparkContext.getOrCreate()\n",
    "glueContext = GlueContext(sparkContext=sparkContext)\n",
    "spark = glueContext.spark_session\n",
    "# logger = glueContext.get_logger()\n",
    "job = Job(glueContext)\n",
    "job.init(args[\"JOB_NAME\"], args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To Print System Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO]\tsys.argv to json: [\n",
      "    \"demo_glue_job.py\",\n",
      "    \"--JOB_ID\",\n",
      "    \"j_a197e5a4ae431ff7631a35bdcddbb8b6bebf747667c548020bfbd40447569b25\",\n",
      "    \"--JOB_RUN_ID\",\n",
      "    \"jr_6a23882f8589db824567693a787ec1014a66dcc868186c2cb8ae26a502f47040\",\n",
      "    \"--JOB_NAME\",\n",
      "    \"TEST_JOB\",\n",
      "    \"--execution_arn\",\n",
      "    \"arn:aws:states:eu-west-1:625904187796:execution:TEST_JOB:try-20240628131134793145\",\n",
      "    \"--job\",\n",
      "    \"job_name\",\n",
      "    \"--stage\",\n",
      "    \"dev\",\n",
      "    \"--log_group_name\",\n",
      "    \"/aws-glue/jobs/demo_glue_job_dev\"\n",
      "]\tjob_name\tjr_6a23882f8589db824567693a787ec1014a66dcc868186c2cb8ae26a502f47040\tarn:aws:states:eu-west-1:625904187796:execution:TEST_JOB:try-20240628131134793145\n",
      "[INFO]\targs to json: {\n",
      "    \"job_bookmark_option\": \"job-bookmark-disable\",\n",
      "    \"job_bookmark_from\": null,\n",
      "    \"job_bookmark_to\": null,\n",
      "    \"JOB_ID\": \"j_a197e5a4ae431ff7631a35bdcddbb8b6bebf747667c548020bfbd40447569b25\",\n",
      "    \"JOB_RUN_ID\": \"jr_6a23882f8589db824567693a787ec1014a66dcc868186c2cb8ae26a502f47040\",\n",
      "    \"SECURITY_CONFIGURATION\": null,\n",
      "    \"encryption_type\": null,\n",
      "    \"enable_data_lineage\": null,\n",
      "    \"RedshiftTempDir\": null,\n",
      "    \"TempDir\": null,\n",
      "    \"JOB_NAME\": \"TEST_JOB\",\n",
      "    \"job\": \"job_name\",\n",
      "    \"execution_arn\": \"arn:aws:states:eu-west-1:625904187796:execution:TEST_JOB:try-20240628131134793145\",\n",
      "    \"log_group_name\": \"/aws-glue/jobs/demo_glue_job_dev\",\n",
      "    \"stage\": \"dev\"\n",
      "}\tjob_name\tjr_6a23882f8589db824567693a787ec1014a66dcc868186c2cb8ae26a502f47040\tarn:aws:states:eu-west-1:625904187796:execution:TEST_JOB:try-20240628131134793145\n"
     ]
    }
   ],
   "source": [
    "logging.info(f\"sys.argv to json: {json.dumps(obj=sys.argv, indent=4)}\")\n",
    "logging.info(f\"args to json: {json.dumps(args, indent=4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To Configure AWS Cloudwatch Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO]\tLog stream 'jr_6a23882f8589db824567693a787ec1014a66dcc868186c2cb8ae26a502f47040' already exists.\tjob_name\tjr_6a23882f8589db824567693a787ec1014a66dcc868186c2cb8ae26a502f47040\tarn:aws:states:eu-west-1:625904187796:execution:TEST_JOB:try-20240628131134793145\n"
     ]
    }
   ],
   "source": [
    "log_utils.create_cloud_watch_log_group()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Business Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO]\tBeginning the job...\tjob_name\tjr_6a23882f8589db824567693a787ec1014a66dcc868186c2cb8ae26a502f47040\tarn:aws:states:eu-west-1:625904187796:execution:TEST_JOB:try-20240628131134793145\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+\n",
      "|Name|Age|\n",
      "+----+---+\n",
      "|John| 28|\n",
      "|Anna| 23|\n",
      "|Mike| 35|\n",
      "|Sara| 29|\n",
      "+----+---+\n",
      "\n",
      "+----+\n",
      "|Name|\n",
      "+----+\n",
      "|John|\n",
      "|Anna|\n",
      "|Mike|\n",
      "|Sara|\n",
      "+----+\n",
      "\n",
      "+----+---+\n",
      "|Name|Age|\n",
      "+----+---+\n",
      "|John| 28|\n",
      "|Mike| 35|\n",
      "|Sara| 29|\n",
      "+----+---+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO]\tEnding the job...\tjob_name\tjr_6a23882f8589db824567693a787ec1014a66dcc868186c2cb8ae26a502f47040\tarn:aws:states:eu-west-1:625904187796:execution:TEST_JOB:try-20240628131134793145\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "|Age|count|\n",
      "+---+-----+\n",
      "| 28|    1|\n",
      "| 23|    1|\n",
      "| 35|    1|\n",
      "| 29|    1|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    logger.info(f\"Beginning the job...\")\n",
    "\n",
    "    data = [(\"John\", 28), (\"Anna\", 23), (\"Mike\", 35), (\"Sara\", 29)]\n",
    "\n",
    "    # Create DataFrame from data\n",
    "    columns = [\"Name\", \"Age\"]\n",
    "    \n",
    "    df = spark.createDataFrame(data, columns)\n",
    "\n",
    "    # Show DataFrame\n",
    "    df.show()\n",
    "\n",
    "    # Select specific columns\n",
    "    df.select(\"Name\").show()\n",
    "\n",
    "    # Filter rows based on conditions\n",
    "    df.filter(df[\"Age\"] > 25).show()\n",
    "\n",
    "    # Group by a column and aggregate data\n",
    "    df.groupBy(\"Age\").count().show()\n",
    "\n",
    "    logger.info(\"Ending the job...\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error encountered during the job: {str(e)}\", exc_info=True)\n",
    "finally:\n",
    "    job.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the SparkSession\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AWS Glue Jupyter Local",
   "language": "python",
   "name": "aws_glue_jupyter_local"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
