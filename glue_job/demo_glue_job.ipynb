{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Local Development prerequisites\n",
    "\n",
    "- Run `source ./setup_aws_glue_scripts/setup_aws_glue.sh` in GitBash\n",
    "\n",
    "AWS Glue Local Debugging:\n",
    "> Run `./glue_local_debug.sh` or follow the following steps.\n",
    "\n",
    "- From the Command Palette (`Ctrl+Shift+P`), select `Tasks: Run Task command`, then select `Convert Notebook to Script`\n",
    "- Run `pipenv shell` in terminal\n",
    "- Run `jupyter notebook --no-browser` in terminal\n",
    "- Copy Jupyter Server url from terminal\n",
    "- From the Command Palette (`Ctrl+Shift+P`), select `Notebook: Select Notebook Kernel`, then select `Select Another Kernel...`, then select `Existing Jupyter Server...`, then paste Jupyter Server url, then enter, then enter again.\n",
    "\n",
    "AWS Glue Interactive Session Debugging:\n",
    "- From the Command Palette (`Ctrl+Shift+P`), select `Tasks: Run Task command`, then select `Convert Notebook to Script`\n",
    "- Run `pipenv shell` in terminal\n",
    "- Run `jupyter notebook --no-browser` in terminal\n",
    "- Copy Jupyter Server url from terminal\n",
    "- From the Command Palette (`Ctrl+Shift+P`), select `Notebook: Select Notebook Kernel`, then select `Select Another Kernel...`, then select `Existing Jupyter Server...`, then paste Jupyter Server url, then enter, then enter again.\n",
    "- Uncomment the following cell, and run it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To configure AWS Glue Interactive Session, uncomment the following cell\n",
    "\n",
    "Change configuration in terms of your requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %profile [your-profile-name]\n",
    "# %region eu-west-1\n",
    "# %iam_role [your-iam-role]\n",
    "# %worker_type G.1X\n",
    "# %number_of_workers 2\n",
    "# %additional_python_modules watchtower\n",
    "# %extra_py_files s3://artifacts-dev/33333/Layers/pip_common_packages_layer_glue.zip,s3://artifacts-dev/33333/Layers/combined_lambda_layer_glue.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To pass system arguments, uncomment the following cell\n",
    "\n",
    "Change arguments in terms of your requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "stage = \"dev\"\n",
    "current_time = datetime.now()\n",
    "\n",
    "sys.argv = [\n",
    "    \"demo_glue_job.py\",  # sys.argv[0], script name\n",
    "    \"true\",\n",
    "    \"--is_local\",\n",
    "    \"true\",\n",
    "    \"--job-bookmark-option\",\n",
    "    \"job-bookmark-disable\",\n",
    "    \"--JOB_ID\",\n",
    "    \"j_a197e5a4ae431ff7631a35bdcddbb8b6bebf747667c548020bfbd40447569b25\",\n",
    "    \"true\",\n",
    "    \"--stage\",\n",
    "    \"dev\",\n",
    "    \"--JOB_RUN_ID\",\n",
    "    \"jr_6a23882f8589db824567693a787ec1014a66dcc868186c2cb8ae26a502f47040\",\n",
    "    \"--JOB_NAME\",\n",
    "    \"TEST_JOB\",\n",
    "    \"--execution_arn\",\n",
    "    f\"arn:aws:states:eu-west-1:625904187796:execution:TEST_JOB:try-{current_time.strftime('%Y%m%d%H%M%S%f')}\",\n",
    "    \"--job\",\n",
    "    \"job_name\",\n",
    "    \"--debugging\",\n",
    "    \"True\",\n",
    "    \"--stage\",\n",
    "    stage,\n",
    "    \"--log_group_name\",\n",
    "    f\"/aws-glue/jobs/demo_glue_job_{stage}\",\n",
    "    \"--total_records\",\n",
    "    \"1000\",\n",
    "    \"--s3_file_path\",\n",
    "    \"test\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize AWS Glue Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Jun 03, 2024 10:29:17 AM org.apache.spark.launcher.Log4jHotPatchOption staticJavaAgentOption\n",
      "WARNING: spark.log4jHotPatch.enabled is set to true, but /usr/share/log4j-cve-2021-44228-hotpatch/jdk17/Log4jHotPatchFat.jar does not exist at the configured location\n",
      "\n",
      "log4j:WARN No appenders could be found for logger (org.apache.spark.util.Utils).\n",
      "log4j:WARN Please initialize the log4j system properly.\n",
      "log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.\n",
      "/Users/kenanhancer/spark-3.3.0-amzn-1-bin-3.3.3-amzn-0/python/pyspark/sql/context.py:112: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exception while setting endpoint config: java.lang.NullPointerException\n",
      "sys.argv to dictionary\n",
      "**********************\n",
      "[\n",
      "    \"demo_glue_job.py\",\n",
      "    \"true\",\n",
      "    \"--is_local\",\n",
      "    \"true\",\n",
      "    \"--job-bookmark-option\",\n",
      "    \"job-bookmark-disable\",\n",
      "    \"--JOB_ID\",\n",
      "    \"j_a197e5a4ae431ff7631a35bdcddbb8b6bebf747667c548020bfbd40447569b25\",\n",
      "    \"true\",\n",
      "    \"--stage\",\n",
      "    \"dev\",\n",
      "    \"--JOB_RUN_ID\",\n",
      "    \"jr_6a23882f8589db824567693a787ec1014a66dcc868186c2cb8ae26a502f47040\",\n",
      "    \"--JOB_NAME\",\n",
      "    \"TEST_JOB\",\n",
      "    \"--execution_arn\",\n",
      "    \"arn:aws:states:eu-west-1:625904187796:execution:TEST_JOB:try-20240603102914598418\",\n",
      "    \"--job\",\n",
      "    \"job_name\",\n",
      "    \"--debugging\",\n",
      "    \"True\",\n",
      "    \"--stage\",\n",
      "    \"dev\",\n",
      "    \"--log_group_name\",\n",
      "    \"/aws-glue/jobs/demo_glue_job_dev\",\n",
      "    \"--total_records\",\n",
      "    \"1000\",\n",
      "    \"--s3_file_path\",\n",
      "    \"test\"\n",
      "]\n",
      "args to dictionary\n",
      "**********************\n",
      "{\n",
      "    \"is_local\": true,\n",
      "    \"job_bookmark_option\": \"job-bookmark-disable\",\n",
      "    \"JOB_ID\": \"j_a197e5a4ae431ff7631a35bdcddbb8b6bebf747667c548020bfbd40447569b25\",\n",
      "    \"stage\": \"dev\",\n",
      "    \"JOB_RUN_ID\": \"jr_6a23882f8589db824567693a787ec1014a66dcc868186c2cb8ae26a502f47040\",\n",
      "    \"JOB_NAME\": \"TEST_JOB\",\n",
      "    \"execution_arn\": \"arn:aws:states:eu-west-1:625904187796:execution:TEST_JOB:try-20240603102914598418\",\n",
      "    \"job\": \"job_name\",\n",
      "    \"debugging\": true,\n",
      "    \"log_group_name\": \"/aws-glue/jobs/demo_glue_job_dev\",\n",
      "    \"total_records\": \"1000\",\n",
      "    \"s3_file_path\": \"test\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import json\n",
    "import logging\n",
    "\n",
    "from glue_utils import argv_to_dict, log_operation\n",
    "from log_utils import LogUtils\n",
    "\n",
    "from awsglue.transforms import *\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from pyspark.context import SparkContext\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.job import Job\n",
    "\n",
    "\n",
    "# Get parameters passed to the script\n",
    "args = getResolvedOptions(\n",
    "    sys.argv,\n",
    "    [\n",
    "        \"JOB_NAME\",\n",
    "        \"job\",\n",
    "        \"execution_arn\",\n",
    "        \"debugging\",\n",
    "        \"log_group_name\",\n",
    "        \"total_records\",\n",
    "        \"stage\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "log_utils = LogUtils(\n",
    "    log_group_name=args[\"log_group_name\"],\n",
    "    log_stream_name=args[\"JOB_RUN_ID\"],\n",
    "    job_name=args[\"job\"],\n",
    "    execution_arn=args[\"execution_arn\"],\n",
    ")\n",
    "\n",
    "log_utils.configure_logging()\n",
    "\n",
    "logger = logging.getLogger()\n",
    "\n",
    "# Initialize Glue context\n",
    "sparkContext = SparkContext.getOrCreate()\n",
    "glueContext = GlueContext(sparkContext=sparkContext)\n",
    "spark = glueContext.spark_session\n",
    "# logger = glueContext.get_logger()\n",
    "job = Job(glueContext)\n",
    "job.init(args[\"JOB_NAME\"], args)\n",
    "\n",
    "\n",
    "print(\"sys.argv to dictionary\")\n",
    "print(\"**********************\")\n",
    "print(json.dumps(obj=sys.argv, indent=4))\n",
    "\n",
    "args = argv_to_dict(argv=sys.argv)\n",
    "\n",
    "args_json = json.dumps(args, indent=4)\n",
    "\n",
    "print(\"args to dictionary\")\n",
    "print(\"**********************\")\n",
    "print(args_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To Configure AWS Cloudwatch Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_utils.create_cloud_watch_log_group()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Business Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Beginning the job...\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+\n",
      "|Name|Age|\n",
      "+----+---+\n",
      "|John| 28|\n",
      "|Anna| 23|\n",
      "|Mike| 35|\n",
      "|Sara| 29|\n",
      "+----+---+\n",
      "\n",
      "+----+\n",
      "|Name|\n",
      "+----+\n",
      "|John|\n",
      "|Anna|\n",
      "|Mike|\n",
      "|Sara|\n",
      "+----+\n",
      "\n",
      "+----+---+\n",
      "|Name|Age|\n",
      "+----+---+\n",
      "|John| 28|\n",
      "|Mike| 35|\n",
      "|Sara| 29|\n",
      "+----+---+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Ending the job...                                                     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "|Age|count|\n",
      "+---+-----+\n",
      "| 28|    1|\n",
      "| 23|    1|\n",
      "| 35|    1|\n",
      "| 29|    1|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    logger.info(f\"Beginning the job...\")\n",
    "\n",
    "    data = [(\"John\", 28), (\"Anna\", 23), (\"Mike\", 35), (\"Sara\", 29)]\n",
    "\n",
    "    # Create DataFrame from data\n",
    "    columns = [\"Name\", \"Age\"]\n",
    "    \n",
    "    df = spark.createDataFrame(data, columns)\n",
    "\n",
    "    # Show DataFrame\n",
    "    df.show()\n",
    "\n",
    "    # Select specific columns\n",
    "    df.select(\"Name\").show()\n",
    "\n",
    "    # Filter rows based on conditions\n",
    "    df.filter(df[\"Age\"] > 25).show()\n",
    "\n",
    "    # Group by a column and aggregate data\n",
    "    df.groupBy(\"Age\").count().show()\n",
    "\n",
    "    logger.info(\"Ending the job...\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error encountered during the job: {str(e)}\", exc_info=True)\n",
    "finally:\n",
    "    job.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the SparkSession\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AWS Glue Jupyter Local",
   "language": "python",
   "name": "aws_glue_jupyter_local"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
