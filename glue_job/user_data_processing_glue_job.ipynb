{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Local Development prerequisites\n",
    "\n",
    "- Run `source ./setup_aws_glue_scripts/setup_aws_glue.sh` in GitBash\n",
    "\n",
    "AWS Glue Local Debugging:\n",
    "> Run `./glue_local_debug.sh` or follow the following steps.\n",
    "\n",
    "- From the Command Palette (`Ctrl+Shift+P`), select `Tasks: Run Task command`, then select `Convert Notebook to Script`\n",
    "- Run `pipenv shell` in terminal\n",
    "- Run `jupyter notebook --no-browser` in terminal\n",
    "- Copy Jupyter Server url from terminal\n",
    "- From the Command Palette (`Ctrl+Shift+P`), select `Notebook: Select Notebook Kernel`, then select `Select Another Kernel...`, then select `Existing Jupyter Server...`, then paste Jupyter Server url, then enter, then enter again.\n",
    "\n",
    "AWS Glue Interactive Session Debugging:\n",
    "- From the Command Palette (`Ctrl+Shift+P`), select `Tasks: Run Task command`, then select `Convert Notebook to Script`\n",
    "- Run `pipenv shell` in terminal\n",
    "- Run `jupyter notebook --no-browser` in terminal\n",
    "- Copy Jupyter Server url from terminal\n",
    "- From the Command Palette (`Ctrl+Shift+P`), select `Notebook: Select Notebook Kernel`, then select `Select Another Kernel...`, then select `Existing Jupyter Server...`, then paste Jupyter Server url, then enter, then enter again.\n",
    "- Uncomment the following cell, and run it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To configure AWS Glue Interactive Session, uncomment the following cell\n",
    "\n",
    "Change configuration in terms of your requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %profile [your-profile-name]\n",
    "# %region eu-west-1\n",
    "# %iam_role [your-iam-role]\n",
    "# %worker_type G.1X\n",
    "# %number_of_workers 2\n",
    "# %additional_python_modules watchtower\n",
    "# %extra_py_files s3://artifacts-dev/33333/Layers/pip_common_packages_layer_glue.zip,s3://artifacts-dev/33333/Layers/combined_lambda_layer_glue.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To pass system arguments, uncomment the following cell\n",
    "\n",
    "Change arguments in terms of your requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "stage = \"dev\"\n",
    "current_time = datetime.now()\n",
    "\n",
    "sys.argv = [\n",
    "    \"user_data_processing_glue_job.py\",  # sys.argv[0], script name\n",
    "    \"true\",\n",
    "    \"--is_local\",\n",
    "    \"true\",\n",
    "    \"--job-bookmark-option\",\n",
    "    \"job-bookmark-disable\",\n",
    "    \"--JOB_ID\",\n",
    "    \"j_a197e5a4ae431ff7631a35bdcddbb8b6bebf747667c548020bfbd40447569b25\",\n",
    "    \"true\",\n",
    "    \"--stage\",\n",
    "    \"dev\",\n",
    "    \"--JOB_RUN_ID\",\n",
    "    \"jr_6a23882f8589db824567693a787ec1014a66dcc868186c2cb8ae26a502f47040\",\n",
    "    \"--JOB_NAME\",\n",
    "    \"TEST_JOB\",\n",
    "    \"--execution_arn\",\n",
    "    f\"arn:aws:states:eu-west-1:625904187796:execution:TEST_JOB:try-{current_time.strftime('%Y%m%d%H%M%S%f')}\",\n",
    "    \"--job\",\n",
    "    \"job_name\",\n",
    "    \"--debugging\",\n",
    "    \"True\",\n",
    "    \"--stage\",\n",
    "    stage,\n",
    "    \"--log_group_name\",\n",
    "    f\"/aws-glue/jobs/user_data_processing_glue_job_{stage}\",\n",
    "    \"--total_records\",\n",
    "    \"1000\",\n",
    "    \"--s3_file_path\",\n",
    "    \"test\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize AWS Glue Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Jun 02, 2024 4:28:36 PM org.apache.spark.launcher.Log4jHotPatchOption staticJavaAgentOption\n",
      "WARNING: spark.log4jHotPatch.enabled is set to true, but /usr/share/log4j-cve-2021-44228-hotpatch/jdk17/Log4jHotPatchFat.jar does not exist at the configured location\n",
      "\n",
      "log4j:WARN No appenders could be found for logger (org.apache.spark.util.Utils).\n",
      "log4j:WARN Please initialize the log4j system properly.\n",
      "log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.\n",
      "/Users/kenanhancer/spark-3.3.0-amzn-1-bin-3.3.3-amzn-0/python/pyspark/sql/context.py:112: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exception while setting endpoint config: java.lang.NullPointerException\n",
      "sys.argv to dictionary\n",
      "**********************\n",
      "[\n",
      "    \"user_data_processing_glue_job.py\",\n",
      "    \"true\",\n",
      "    \"--is_local\",\n",
      "    \"true\",\n",
      "    \"--job-bookmark-option\",\n",
      "    \"job-bookmark-disable\",\n",
      "    \"--JOB_ID\",\n",
      "    \"j_a197e5a4ae431ff7631a35bdcddbb8b6bebf747667c548020bfbd40447569b25\",\n",
      "    \"true\",\n",
      "    \"--stage\",\n",
      "    \"dev\",\n",
      "    \"--JOB_RUN_ID\",\n",
      "    \"jr_6a23882f8589db824567693a787ec1014a66dcc868186c2cb8ae26a502f47040\",\n",
      "    \"--JOB_NAME\",\n",
      "    \"TEST_JOB\",\n",
      "    \"--execution_arn\",\n",
      "    \"arn:aws:states:eu-west-1:625904187796:execution:TEST_JOB:try-20240602162833647153\",\n",
      "    \"--job\",\n",
      "    \"job_name\",\n",
      "    \"--debugging\",\n",
      "    \"True\",\n",
      "    \"--stage\",\n",
      "    \"dev\",\n",
      "    \"--log_group_name\",\n",
      "    \"/aws-glue/jobs/user_data_processing_glue_job_dev\",\n",
      "    \"--total_records\",\n",
      "    \"1000\",\n",
      "    \"--s3_file_path\",\n",
      "    \"test\"\n",
      "]\n",
      "args to dictionary\n",
      "**********************\n",
      "{\n",
      "    \"is_local\": true,\n",
      "    \"job_bookmark_option\": \"job-bookmark-disable\",\n",
      "    \"JOB_ID\": \"j_a197e5a4ae431ff7631a35bdcddbb8b6bebf747667c548020bfbd40447569b25\",\n",
      "    \"stage\": \"dev\",\n",
      "    \"JOB_RUN_ID\": \"jr_6a23882f8589db824567693a787ec1014a66dcc868186c2cb8ae26a502f47040\",\n",
      "    \"JOB_NAME\": \"TEST_JOB\",\n",
      "    \"execution_arn\": \"arn:aws:states:eu-west-1:625904187796:execution:TEST_JOB:try-20240602162833647153\",\n",
      "    \"job\": \"job_name\",\n",
      "    \"debugging\": true,\n",
      "    \"log_group_name\": \"/aws-glue/jobs/user_data_processing_glue_job_dev\",\n",
      "    \"total_records\": \"1000\",\n",
      "    \"s3_file_path\": \"test\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import json\n",
    "import time\n",
    "import logging\n",
    "\n",
    "from glue_utils import argv_to_dict\n",
    "\n",
    "from awsglue.transforms import *\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from pyspark.context import SparkContext\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.job import Job\n",
    "\n",
    "\n",
    "# Get parameters passed to the script\n",
    "args = getResolvedOptions(\n",
    "    sys.argv,\n",
    "    [\n",
    "        \"JOB_NAME\",\n",
    "        \"job\",\n",
    "        \"execution_arn\",\n",
    "        \"debugging\",\n",
    "        \"log_group_name\",\n",
    "        \"total_records\",\n",
    "        \"stage\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Initialize Glue context\n",
    "sparkContext = SparkContext.getOrCreate()\n",
    "glueContext = GlueContext(sparkContext=sparkContext)\n",
    "spark = glueContext.spark_session\n",
    "logger = glueContext.get_logger()\n",
    "job = Job(glueContext)\n",
    "job.init(args[\"JOB_NAME\"], args)\n",
    "\n",
    "\n",
    "# # Configure logging to display messages in Jupyter notebook\n",
    "# logging.basicConfig(\n",
    "#     level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    "# )\n",
    "\n",
    "# # Ensure logger from GlueContext outputs to Jupyter notebook\n",
    "# if not logger.handlers:\n",
    "#     stream_handler = logging.StreamHandler(sys.stdout)\n",
    "#     stream_handler.setLevel(logging.INFO)\n",
    "#     stream_handler.setFormatter(\n",
    "#         logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "#     )\n",
    "#     logger.addHandler(stream_handler)\n",
    "\n",
    "print(\"sys.argv to dictionary\")\n",
    "print(\"**********************\")\n",
    "print(json.dumps(obj=sys.argv, indent=4))\n",
    "\n",
    "args = argv_to_dict(argv=sys.argv)\n",
    "\n",
    "args_json = json.dumps(args, indent=4)\n",
    "\n",
    "print(\"args to dictionary\")\n",
    "print(\"**********************\")\n",
    "print(args_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_operation(operation_name, func):\n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        logger.info(f\"Starting operation: {operation_name}\")\n",
    "        func()\n",
    "        logger.info(\n",
    "            f\"Completed operation: {operation_name} in {time.time() - start_time:.2f} seconds\"\n",
    "        )\n",
    "    except Exception as e:\n",
    "        logger.error(\n",
    "            f\"Error during operation: {operation_name} - {str(e)}\", exc_info=True\n",
    "        )\n",
    "\n",
    "\n",
    "def read_csv_file(file_path):\n",
    "    global user_data_df\n",
    "    user_data_df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "    logger.info(f\"DataFrame loaded from {file_path}:\")\n",
    "    user_data_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To configure AWS Cloudwatch Logging(Mandatory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from setup_cloudwatch_logging import setup_cloudwatch_logging\n",
    "\n",
    "\n",
    "setup_cloudwatch_logging(\n",
    "    log_group_name=args[\"log_group_name\"],\n",
    "    log_stream_name=args[\"JOB_RUN_ID\"],\n",
    "    job_name=args[\"job\"],\n",
    "    execution_arn=args[\"execution_arn\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Business Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+-------------+\n",
      "|   name|age|         city|\n",
      "+-------+---+-------------+\n",
      "|  Alice| 30|     New York|\n",
      "|    Bob| 25|  Los Angeles|\n",
      "|Charlie| 35|      Chicago|\n",
      "|  Diana| 28|San Francisco|\n",
      "|    Eve| 32|       Boston|\n",
      "|  Frank| 27|      Seattle|\n",
      "|  Grace| 29|       Austin|\n",
      "|   Hank| 31|       Denver|\n",
      "|  Irene| 33|      Phoenix|\n",
      "|   Jack| 26|    San Diego|\n",
      "|  Karen| 34|       Dallas|\n",
      "|    Leo| 28|     San Jose|\n",
      "|   Mona| 30| Indianapolis|\n",
      "|   Nate| 25|     Columbus|\n",
      "| Olivia| 33|    Charlotte|\n",
      "|   Paul| 29|     Portland|\n",
      "| Quincy| 35|    Las Vegas|\n",
      "| Rachel| 28|Oklahoma City|\n",
      "|  Steve| 32|   Louisville|\n",
      "|   Tina| 27|    Baltimore|\n",
      "+-------+---+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def read_user_data_csv():\n",
    "    global user_data_df\n",
    "    csv_file_path = \"./glue_job/user_data.csv\"\n",
    "    user_data_df = spark.read.csv(csv_file_path, header=True, inferSchema=True)\n",
    "    logger.info(f\"CSV file successfully read from path: {csv_file_path}\")\n",
    "    user_data_df.show()\n",
    "\n",
    "\n",
    "log_operation(\"Reading User Data CSV\", read_user_data_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_users_older_than_30():\n",
    "    global filtered_user_data_df\n",
    "    filtered_user_data_df = user_data_df.filter(user_data_df[\"age\"] > 30)\n",
    "    logger.info(\"Users older than 30 have been selected.\")\n",
    "    filtered_user_data_df.show()\n",
    "\n",
    "\n",
    "log_operation(\"Filtering Users Older Than 30\", filter_users_older_than_30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_name_and_city_columns():\n",
    "    global selected_user_data_df\n",
    "    selected_user_data_df = filtered_user_data_df.select(\"name\", \"city\")\n",
    "    logger.info(\"'name' and 'city' columns have been selected.\")\n",
    "    selected_user_data_df.show()\n",
    "\n",
    "\n",
    "log_operation(\"Selecting Name and City Columns\", select_name_and_city_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the SparkSession\n",
    "job.commit()\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AWS Glue Jupyter Local",
   "language": "python",
   "name": "aws_glue_jupyter_local"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
