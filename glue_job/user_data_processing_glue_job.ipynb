{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Local Development prerequisites\n",
    "\n",
    "- Run `source ./setup_aws_glue_scripts/setup_aws_glue.sh` in GitBash\n",
    "\n",
    "AWS Glue Local Debugging:\n",
    "> Run `./glue_local_debug.sh` or follow the following steps.\n",
    "\n",
    "- From the Command Palette (`Ctrl+Shift+P`), select `Tasks: Run Task command`, then select `Convert Notebook to Script`\n",
    "- Run `pipenv shell` in terminal\n",
    "- Run `jupyter notebook --no-browser` in terminal\n",
    "- Copy Jupyter Server url from terminal\n",
    "- From the Command Palette (`Ctrl+Shift+P`), select `Notebook: Select Notebook Kernel`, then select `Select Another Kernel...`, then select `Existing Jupyter Server...`, then paste Jupyter Server url, then enter, then enter again.\n",
    "\n",
    "AWS Glue Interactive Session Debugging:\n",
    "- From the Command Palette (`Ctrl+Shift+P`), select `Tasks: Run Task command`, then select `Convert Notebook to Script`\n",
    "- Run `pipenv shell` in terminal\n",
    "- Run `jupyter notebook --no-browser` in terminal\n",
    "- Copy Jupyter Server url from terminal\n",
    "- From the Command Palette (`Ctrl+Shift+P`), select `Notebook: Select Notebook Kernel`, then select `Select Another Kernel...`, then select `Existing Jupyter Server...`, then paste Jupyter Server url, then enter, then enter again.\n",
    "- Uncomment the following cell, and run it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To configure AWS Glue Interactive Session, uncomment the following cell\n",
    "\n",
    "Change configuration in terms of your requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %profile [your-profile-name]\n",
    "# %region eu-west-1\n",
    "# %iam_role [your-iam-role]\n",
    "# %worker_type G.1X\n",
    "# %number_of_workers 2\n",
    "# %additional_python_modules watchtower\n",
    "# %extra_py_files s3://artifacts-dev/33333/Layers/pip_common_packages_layer_glue.zip,s3://artifacts-dev/33333/Layers/combined_lambda_layer_glue.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To pass system arguments, uncomment the following cell\n",
    "\n",
    "Change arguments in terms of your requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "stage = os.environ.get(\"STAGE\", \"dev\")\n",
    "current_time = datetime.now()\n",
    "\n",
    "sys.argv = [\n",
    "    \"user_data_processing_glue_job.py\",  # sys.argv[0], script name\n",
    "    \"--JOB_ID\", \"j_a197e5a4ae431ff7631a35bdcddbb8b6bebf747667c548020bfbd40447569b25\",\n",
    "    \"--JOB_RUN_ID\", \"jr_6a23882f8589db824567693a787ec1014a66dcc868186c2cb8ae26a502f47040\",\n",
    "    \"--JOB_NAME\", \"TEST_JOB\",\n",
    "    \"--execution_arn\", f\"arn:aws:states:eu-west-1:625904187796:execution:TEST_JOB:try-{current_time.strftime('%Y%m%d%H%M%S%f')}\",\n",
    "    \"--job\", \"job_name\",\n",
    "    \"--stage\", stage,\n",
    "    \"--log_group_name\", f\"/aws-glue/jobs/user_data_processing_glue_job_{stage}\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize AWS Glue Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import logging\n",
    "\n",
    "from log_utils import LogUtils, log_operation\n",
    "\n",
    "from awsglue.transforms import *\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from pyspark.context import SparkContext\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.job import Job\n",
    "\n",
    "\n",
    "# Get parameters passed to the script\n",
    "args = getResolvedOptions(\n",
    "    sys.argv,\n",
    "    [\n",
    "        \"JOB_NAME\",\n",
    "        \"job\",\n",
    "        \"execution_arn\",\n",
    "        \"log_group_name\",\n",
    "        \"stage\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "env = os.environ.get(\"ENV\")\n",
    "log_utils = LogUtils(\n",
    "    log_group_name=args[\"log_group_name\"],\n",
    "    log_stream_name=args[\"JOB_RUN_ID\"],\n",
    "    job_name=args[\"job\"],\n",
    "    execution_arn=args[\"execution_arn\"],\n",
    "    environment=env\n",
    ")\n",
    "\n",
    "log_utils.configure_logging()\n",
    "\n",
    "logger = logging.getLogger()\n",
    "\n",
    "# Initialize Glue context\n",
    "sparkContext = SparkContext.getOrCreate()\n",
    "glueContext = GlueContext(sparkContext=sparkContext)\n",
    "spark = glueContext.spark_session\n",
    "# logger = glueContext.get_logger()\n",
    "job = Job(glueContext)\n",
    "job.init(args[\"JOB_NAME\"], args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To Print System Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(f\"sys.argv to json: {json.dumps(obj=sys.argv, indent=4)}\")\n",
    "logging.info(f\"args to json: {json.dumps(args, indent=4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To Configure AWS Cloudwatch Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_utils.create_cloud_watch_log_group()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Business Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_user_data_csv():\n",
    "    global user_data_df\n",
    "    csv_file_path = \"./glue_job/user_data.csv\"\n",
    "    user_data_df = spark.read.csv(csv_file_path, header=True, inferSchema=True)\n",
    "    logger.info(f\"CSV file successfully read from path: {csv_file_path}\")\n",
    "    user_data_df.show()\n",
    "\n",
    "\n",
    "log_operation(\"Reading User Data CSV\", read_user_data_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_users_older_than_30():\n",
    "    global filtered_user_data_df\n",
    "    filtered_user_data_df = user_data_df.filter(user_data_df[\"age\"] > 30)\n",
    "    logger.info(\"Users older than 30 have been selected.\")\n",
    "    filtered_user_data_df.show()\n",
    "\n",
    "\n",
    "log_operation(\"Filtering Users Older Than 30\", filter_users_older_than_30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_name_and_city_columns():\n",
    "    global selected_user_data_df\n",
    "    selected_user_data_df = filtered_user_data_df.select(\"name\", \"city\")\n",
    "    logger.info(\"'name' and 'city' columns have been selected.\")\n",
    "    selected_user_data_df.show()\n",
    "\n",
    "\n",
    "log_operation(\"Selecting Name and City Columns\", select_name_and_city_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the SparkSession\n",
    "job.commit()\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AWS Glue Jupyter Local",
   "language": "python",
   "name": "aws_glue_jupyter_local"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
